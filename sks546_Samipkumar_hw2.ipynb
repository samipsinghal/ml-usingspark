{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f016dbb-9bf5-42ca-8f8a-c8210a392146",
   "metadata": {},
   "source": [
    "### Question 1 ####\n",
    "\n",
    "# Restart Kernel\n",
    "To ensure a clean state, please restart the kernel before proceeding to the next steps.\n",
    "- In Jupyter Notebook, go to the menu: Kernel -> Restart & Clear Output\n",
    "\n",
    "or run below code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a879f96-236c-47e9-9275-f929aff15a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipykernel.kernelapp import IPKernelApp\n",
    "\n",
    "def restart_kernel():\n",
    "    app = IPKernelApp.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6786c0-8fb8-4756-9c95-d0edc84030ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/13 23:34:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/13 23:34:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/07/13 23:34:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "/opt/conda/envs/bigdata/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Transaction: integer (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      "\n",
      "+----------+-------------------+-----------+-------------+\n",
      "|      Date|               Time|Transaction|         Item|\n",
      "+----------+-------------------+-----------+-------------+\n",
      "|2016-10-30|2024-07-13 09:58:11|          1|        Bread|\n",
      "|2016-10-30|2024-07-13 10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|2024-07-13 10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|2024-07-13 10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|2024-07-13 10:07:57|          3|          Jam|\n",
      "+----------+-------------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Unsorted Result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---+\n",
      "|         Item|       day|qty|\n",
      "+-------------+----------+---+\n",
      "|         Cake|2016-11-06|  1|\n",
      "|       Coffee|2016-11-12| 19|\n",
      "|        Bread|2016-11-28|  5|\n",
      "|         NONE|2016-12-31|  1|\n",
      "|        Fudge|2017-01-30|  1|\n",
      "|        Salad|2017-02-05|  1|\n",
      "|      Cookies|2017-02-09|  5|\n",
      "|Hot chocolate|2017-02-16|  5|\n",
      "|Hot chocolate|2017-02-23|  1|\n",
      "|         Eggs|2017-03-03|  1|\n",
      "|       Muffin|2017-03-05|  3|\n",
      "|     Sandwich|2017-03-16|  1|\n",
      "|        Bread|2017-04-06|  2|\n",
      "|Hot chocolate|2016-11-07|  1|\n",
      "|Mineral water|2016-11-23|  1|\n",
      "|     Art Tray|2016-12-10|  1|\n",
      "|       Pastry|2016-12-12|  1|\n",
      "|        Bread|2016-12-21|  5|\n",
      "|      Brownie|2016-12-23|  2|\n",
      "|        Fudge|2016-12-29|  1|\n",
      "+-------------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Sorted Result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+\n",
      "|                Item|       day|qty|\n",
      "+--------------------+----------+---+\n",
      "|Afternoon with th...|2017-01-21|  2|\n",
      "|Afternoon with th...|2017-01-22|  1|\n",
      "|Afternoon with th...|2017-02-18|  1|\n",
      "|           Alfajores|2016-11-02|  1|\n",
      "|           Alfajores|2016-11-04|  1|\n",
      "|           Alfajores|2016-11-08|  3|\n",
      "|           Alfajores|2016-11-11|  3|\n",
      "|           Alfajores|2016-11-12|  3|\n",
      "|           Alfajores|2016-11-13|  1|\n",
      "|           Alfajores|2016-11-17|  5|\n",
      "|           Alfajores|2016-11-20|  4|\n",
      "|           Alfajores|2016-11-25|  2|\n",
      "|           Alfajores|2016-11-27|  2|\n",
      "|           Alfajores|2016-12-03|  1|\n",
      "|           Alfajores|2016-12-04|  2|\n",
      "|           Alfajores|2016-12-05|  1|\n",
      "|           Alfajores|2016-12-07|  1|\n",
      "|           Alfajores|2016-12-09|  1|\n",
      "|           Alfajores|2016-12-13|  1|\n",
      "|           Alfajores|2016-12-14|  2|\n",
      "+--------------------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, lit, date_format, hour, count, to_timestamp\n",
    "\n",
    "# Reuse the existing Spark session\n",
    "# spark session\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "# conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "\n",
    "# conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "conf.set('spark.driver.memory','4g')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)\n",
    "sc\n",
    "\n",
    "# Load the data\n",
    "file_path = \"shared/hw2/Bakery.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema to understand the data structure\n",
    "df.printSchema()\n",
    "\n",
    "# Display the first few rows to inspect the data\n",
    "df.show(5)\n",
    "\n",
    "# Combine Date and Time into a single timestamp column\n",
    "df = df.withColumn(\"timestamp\", concat(col(\"Date\"), lit(\" \"), date_format(col(\"Time\"), 'HH:mm:ss')))\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Filter data between 11 AM and 1 PM\n",
    "filtered_df = df.filter((hour(col(\"timestamp\")) >= 11) & (hour(col(\"timestamp\")) < 13))\n",
    "\n",
    "# Extract date and group by item and day, then count the occurrences\n",
    "result_df = filtered_df.withColumn(\"day\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")) \\\n",
    "    .groupBy(\"Item\", \"day\") \\\n",
    "    .agg(count(\"Transaction\").alias(\"qty\"))\n",
    "\n",
    "# Display the unsorted result\n",
    "print(\"Unsorted Result:\")\n",
    "result_df.show()\n",
    "\n",
    "# Sort by Item asc, day asc, and qty desc\n",
    "sorted_result_df = result_df.orderBy(col(\"Item\").asc(), col(\"day\").asc(), col(\"qty\").desc())\n",
    "\n",
    "# Display the sorted result\n",
    "print(\"Sorted Result:\")\n",
    "sorted_result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8508ee4-42c6-4ccb-a5bb-fafca1b2c1b7",
   "metadata": {},
   "source": [
    "### Question 2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba3bfc1-a0cb-48f5-9f1e-077bdd027dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------------------------------------+\n",
      "|daypart  |daytype|top_3_items                               |\n",
      "+---------+-------+------------------------------------------+\n",
      "|Afternoon|Weekday|Coffee, Bread, Tea                        |\n",
      "|Afternoon|Weekend|Coffee, Bread, Tea                        |\n",
      "|Evening  |Weekday|Coffee, Bread, Fudge                      |\n",
      "|Evening  |Weekend|Tshirt, Afternoon with the baker, Postcard|\n",
      "|Morning  |Weekday|Coffee, Bread, Pastry                     |\n",
      "|Morning  |Weekend|Coffee, Bread, Pastry                     |\n",
      "|Night    |Weekend|Bread                                     |\n",
      "+---------+-------+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, lit, date_format, hour, count, to_timestamp, when, dayofweek, row_number, collect_list, array_join\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Reuse the existing Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "file_path = \"shared/hw2/Bakery.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Combine Date and Time into a single timestamp column\n",
    "df = df.withColumn(\"timestamp\", concat(col(\"Date\"), lit(\" \"), date_format(col(\"Time\"), 'HH:mm:ss')))\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Define dayparts\n",
    "df = df.withColumn(\"daypart\", \n",
    "                   when((hour(col(\"timestamp\")) >= 6) & (hour(col(\"timestamp\")) < 12), \"Morning\")\n",
    "                   .when((hour(col(\"timestamp\")) >= 12) & (hour(col(\"timestamp\")) < 18), \"Afternoon\")\n",
    "                   .when((hour(col(\"timestamp\")) >= 18) & (hour(col(\"timestamp\")) < 24), \"Evening\")\n",
    "                   .otherwise(\"Night\"))\n",
    "\n",
    "# Define day types\n",
    "df = df.withColumn(\"daytype\", \n",
    "                   when(dayofweek(col(\"timestamp\")).isin([1, 7]), \"Weekend\")\n",
    "                   .otherwise(\"Weekday\"))\n",
    "\n",
    "# Group by item, daypart, and daytype, then count the occurrences\n",
    "grouped_df = df.groupBy(\"Item\", \"daypart\", \"daytype\") \\\n",
    "               .agg(count(\"Transaction\").alias(\"qty\"))\n",
    "\n",
    "# Create a window specification to rank items by qty within each daypart and daytype\n",
    "window_spec = Window.partitionBy(\"daypart\", \"daytype\").orderBy(col(\"qty\").desc())\n",
    "\n",
    "# Add a row number to each row in the window\n",
    "ranked_df = grouped_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the top 3 items for each daypart and daytype\n",
    "top_3_df = ranked_df.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Collect the top 3 items into a single column\n",
    "result_df = top_3_df.groupBy(\"daypart\", \"daytype\") \\\n",
    "                    .agg(array_join(collect_list(col(\"Item\")), \", \").alias(\"top_3_items\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56b1f9-851d-468a-b792-08cbbe286686",
   "metadata": {},
   "source": [
    "### Question 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e78b5e-cbeb-4c01-ae81-6ea58552aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+\n",
      "|rpt_area_desc        |total_entities|\n",
      "+---------------------+--------------+\n",
      "|Bed&Breakfast Home   |4             |\n",
      "|Summer Camps         |4             |\n",
      "|Institutions         |30            |\n",
      "|Local Confinement    |2             |\n",
      "|Mobile Food          |147           |\n",
      "|School Buildings     |89            |\n",
      "|Summer Food          |242           |\n",
      "|Swimming Pools       |420           |\n",
      "|Day Care             |173           |\n",
      "|Tattoo Establishments|36            |\n",
      "|Residential Care     |154           |\n",
      "|Bed&Breakfast Inn    |2             |\n",
      "|Adult Day Care       |5             |\n",
      "|Lodging              |62            |\n",
      "|Food Service         |1093          |\n",
      "+---------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Reuse the existing Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the JSON data\n",
    "file_path = \"shared/hw2/Restaurants_in_Durham_County_NC.json\"\n",
    "df = spark.read.json(file_path)\n",
    "\n",
    "# Group by rpt_area_desc and count the occurrences\n",
    "result_df = df.groupBy(\"fields.rpt_area_desc\").agg(count(\"*\").alias(\"total_entities\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf919b04-6286-4f6e-8386-d5d4587f7822",
   "metadata": {},
   "source": [
    "### Question 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bd743a-7d48-4ce1-ab69-94851bdd35ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+\n",
      "|Country                 |percentage_increase|\n",
      "+------------------------+-------------------+\n",
      "|United Arab Emirates    |76.27926665641841  |\n",
      "|Afghanistan             |63.73220223919031  |\n",
      "|Western Sahara          |54.71315282296692  |\n",
      "|Turks and Caicos Islands|52.128594047562025 |\n",
      "|Cayman Islands          |45.82701410699342  |\n",
      "+------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder.appName(\"PopulationAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the CSV data\n",
    "file_path = \"shared/hw2/populationbycountry19802010millions.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Rename the first column to 'Country' for better clarity\n",
    "df = df.withColumnRenamed(\"_c0\", \"Country\")\n",
    "\n",
    "# Filter out 'World' and region entries\n",
    "regions = [\"World\", \"Africa\", \"Asia\", \"Europe\", \"Latin America and the Caribbean\", \"Oceania\", \"Northern America\"]\n",
    "filtered_df = df.filter(~col(\"Country\").isin(regions))\n",
    "\n",
    "# Calculate the percentage increase in population between 1990 and 2000\n",
    "result_df = filtered_df.withColumn(\"percentage_increase\", ((col(\"2000\").cast(\"float\") - col(\"1990\").cast(\"float\")) / col(\"1990\").cast(\"float\") * 100))\n",
    "\n",
    "# Select relevant columns and sort by percentage increase in descending order\n",
    "sorted_df = result_df.select(\"Country\", \"percentage_increase\").orderBy(col(\"percentage_increase\").desc())\n",
    "\n",
    "# Show the top 5 countries with the biggest percentage increase\n",
    "sorted_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572016b-164d-4975-878a-394b416f8f5d",
   "metadata": {},
   "source": [
    "### Question 5 ###\n",
    "\n",
    "\n",
    "\n",
    "# Restart Kernel\n",
    "To ensure a clean state, please restart the kernel before proceeding to the next steps.\n",
    "- In Jupyter Notebook, go to the menu: Kernel -> Restart & Clear Output\n",
    "\n",
    "or run below code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de1ae65-cbbb-41c6-8106-c4833c75babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipykernel.kernelapp import IPKernelApp\n",
    "\n",
    "def restart_kernel():\n",
    "    app = IPKernelApp.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b48c231-b5b3-47f8-a88b-c28c03322119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/13 23:38:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/13 23:38:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|word      |count|\n",
      "+----------+-----+\n",
      "|government|4724 |\n",
      "|of        |75568|\n",
      "|was       |10795|\n",
      "|year      |3760 |\n",
      "|minister  |1550 |\n",
      "|us        |3549 |\n",
      "|to        |89046|\n",
      "|5         |1778 |\n",
      "|annually  |41   |\n",
      "|does      |1360 |\n",
      "|consume   |30   |\n",
      "|alarmed   |11   |\n",
      "|if        |5983 |\n",
      "|nothing   |361  |\n",
      "|is        |27601|\n",
      "|annual    |356  |\n",
      "|joynews   |2    |\n",
      "|editor    |266  |\n",
      "|george    |145  |\n",
      "|said      |14909|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Define file paths\n",
    "file_paths = [\n",
    "    \"shared/hw2/hw1text/20-01.txt\",\n",
    "    \"shared/hw2/hw1text/20-02.txt\",\n",
    "    \"shared/hw2/hw1text/20-03.txt\",\n",
    "    \"shared/hw2/hw1text/20-04.txt\",\n",
    "    \"shared/hw2/hw1text/20-05.txt\"\n",
    "]\n",
    "\n",
    "# Read files into an RDD\n",
    "rdd = spark.sparkContext.textFile(','.join(file_paths))\n",
    "\n",
    "# Function to clean and split text\n",
    "def clean_text(line):\n",
    "    # Normalize to lower case\n",
    "    line = line.lower()\n",
    "    # Replace characters not in the set [0-9a-z] with space\n",
    "    line = re.sub(r'[^0-9a-z]', ' ', line)\n",
    "    # Split the line into words\n",
    "    return line.split()\n",
    "\n",
    "# Apply the cleaning function and perform word count\n",
    "word_counts = rdd.flatMap(clean_text) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Convert to DataFrame for better visualization and usage\n",
    "word_count_df = word_counts.toDF([\"word\", \"count\"])\n",
    "\n",
    "# Show the DataFrame (or you can save it to a file)\n",
    "word_count_df.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa54118e-61e9-4258-98e0-4eb33105211f",
   "metadata": {},
   "source": [
    "### Question 6  ###\n",
    "\n",
    "\n",
    "\n",
    "# Restart Kernel\n",
    "To ensure a clean state, please restart the kernel before proceeding to the next steps.\n",
    "- In Jupyter Notebook, go to the menu: Kernel -> Restart & Clear Output\n",
    "\n",
    "or run below code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413d2b42-ec1e-46a6-82a9-8939df0f2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipykernel.kernelapp import IPKernelApp\n",
    "\n",
    "def restart_kernel():\n",
    "    app = IPKernelApp.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d67ddb1-0761-4558-a0e6-3f6fd71fca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/13 23:39:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/13 23:39:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|bigram     |count|\n",
      "+-----------+-----+\n",
      "|{of, the}  |17498|\n",
      "|{in, the}  |12819|\n",
      "|{covid, 19}|8762 |\n",
      "|{to, the}  |8397 |\n",
      "|{for, the} |5592 |\n",
      "|{on, the}  |5043 |\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "from operator import add\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"BigramCount\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Define file paths\n",
    "file_paths = [\n",
    "    \"shared/hw2/hw1text/20-01.txt\",\n",
    "    \"shared/hw2/hw1text/20-02.txt\",\n",
    "    \"shared/hw2/hw1text/20-03.txt\",\n",
    "    \"shared/hw2/hw1text/20-04.txt\",\n",
    "    \"shared/hw2/hw1text/20-05.txt\"\n",
    "]\n",
    "\n",
    "# Read files into an RDD\n",
    "rdd = spark.sparkContext.textFile(','.join(file_paths))\n",
    "\n",
    "# Function to clean and split text into bigrams\n",
    "def clean_and_split_to_bigrams(line):\n",
    "    # Normalize to lower case\n",
    "    line = line.lower()\n",
    "    # Replace characters not in the set [0-9a-z] with space\n",
    "    line = re.sub(r'[^0-9a-z\\s]', ' ', line)\n",
    "    # Split the line into words\n",
    "    words = line.split()\n",
    "    # Remove single-character words except meaningful ones like 'i', 'a'\n",
    "    words = [word for word in words if len(word) > 1 or word in ('i', 'a')]\n",
    "    # Generate bigrams\n",
    "    bigrams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "    return bigrams\n",
    "\n",
    "# Apply the cleaning and bigram splitting function\n",
    "bigrams_rdd = rdd.flatMap(clean_and_split_to_bigrams)\n",
    "\n",
    "# Map bigrams to key-value pairs and reduce by key to count occurrences\n",
    "bigram_counts = bigrams_rdd.map(lambda bigram: (bigram, 1)).reduceByKey(add)\n",
    "\n",
    "# Convert to DataFrame for better visualization and usage\n",
    "bigram_count_df = bigram_counts.toDF([\"bigram\", \"count\"])\n",
    "\n",
    "# Get the 6 most common bigrams\n",
    "most_common_bigrams = bigram_count_df.orderBy(\"count\", ascending=False).limit(6)\n",
    "\n",
    "# Show the most common bigrams\n",
    "most_common_bigrams.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33207e83-23bc-41be-827b-56537c054b7a",
   "metadata": {},
   "source": [
    "### Question 7 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66df9051-dbf2-47e2-a6e7-ff758dc89a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest Restaurant:\n",
      "Name: OLD HAVANA SANDWICH SHOP\n",
      "Address: 310 E. MAIN ST., DURHAM, NC 27701\n",
      "Number of foreclosures within 1 mile of OLD HAVANA SANDWICH SHOP: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "# Load restaurant data\n",
    "with open('shared/hw2/Restaurants_in_Durham_County_NC.json', 'r') as file:\n",
    "    restaurant_data = json.load(file)\n",
    "\n",
    "# Filter active food service restaurants\n",
    "active_food_service_restaurants = [\n",
    "    restaurant for restaurant in restaurant_data\n",
    "    if restaurant['fields'].get('status') == 'ACTIVE' and restaurant['fields'].get('rpt_area_desc') == 'Food Service'\n",
    "]\n",
    "\n",
    "# Reference coordinates\n",
    "reference_coords = (35.994914, -78.897133)\n",
    "\n",
    "# Find the closest restaurant\n",
    "closest_restaurant = None\n",
    "min_distance = float('inf')\n",
    "\n",
    "for restaurant in active_food_service_restaurants:\n",
    "    if 'geolocation' in restaurant['fields']:\n",
    "        coords = restaurant['fields']['geolocation']\n",
    "        restaurant_coords = (coords[0], coords[1])\n",
    "        distance = haversine(reference_coords, restaurant_coords, unit=Unit.MILES)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_restaurant = restaurant\n",
    "\n",
    "if closest_restaurant:\n",
    "    # Display closest restaurant\n",
    "    restaurant_name = closest_restaurant['fields']['premise_name']\n",
    "    restaurant_address = closest_restaurant['fields']['premise_address1']\n",
    "    restaurant_city = closest_restaurant['fields']['premise_city']\n",
    "    restaurant_zip = closest_restaurant['fields']['premise_zip']\n",
    "    print(\"Closest Restaurant:\")\n",
    "    print(f\"Name: {restaurant_name}\")\n",
    "    print(f\"Address: {restaurant_address}, {restaurant_city}, NC {restaurant_zip}\")\n",
    "\n",
    "    # Load foreclosure data\n",
    "    with open('shared/hw2/durham-nc-foreclosure-2006-2016.json', 'r') as file:\n",
    "        foreclosure_data = json.load(file)\n",
    "\n",
    "    # Extract foreclosure coordinates\n",
    "    foreclosure_coords = [\n",
    "        (foreclosure['fields']['geocode'][1], foreclosure['fields']['geocode'][0])\n",
    "        for foreclosure in foreclosure_data if 'geocode' in foreclosure['fields']\n",
    "    ]\n",
    "\n",
    "    # Get the coordinates of the closest restaurant\n",
    "    restaurant_coords = (closest_restaurant['fields']['geolocation'][0], closest_restaurant['fields']['geolocation'][1])\n",
    "\n",
    "    # Count foreclosures within 1 mile of the closest restaurant\n",
    "    foreclosure_count = sum(\n",
    "        1 for coords in foreclosure_coords\n",
    "        if haversine(coords, restaurant_coords, unit=Unit.MILES) <= 1\n",
    "    )\n",
    "\n",
    "    print(f\"Number of foreclosures within 1 mile of {restaurant_name}: {foreclosure_count}\")\n",
    "else:\n",
    "    print(\"No active food service restaurants found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c83b05-e441-4868-b6f8-9de29e7a1234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
